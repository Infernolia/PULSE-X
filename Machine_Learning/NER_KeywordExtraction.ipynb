{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Saloni_Final.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMHhIwzM3hAtl2RjMl2sUkz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0Fx_ATpbtYeJ","colab_type":"text"},"source":["##Product Name Using DeepPavlov\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0DLzgHbYOrnK","colab_type":"text"},"source":["#Installations"]},{"cell_type":"code","metadata":{"id":"A9Xv0ujOODoM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1601036239782,"user_tz":-330,"elapsed":104420,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"47714ff0-4a95-4a4e-916e-bb343d80b9ec"},"source":["!pip install deeppavlov\n","!python -m deeppavlov install ner_ontonotes_bert #Fucks with pandas version\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting deeppavlov\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/f6/df4ce4c5c5cafd8d357a4c02cb1ccb5ff1d8f3c21de3e5d02299eef56342/deeppavlov-0.12.1-py3-none-any.whl (948kB)\n","\u001b[K     |████████████████████████████████| 952kB 2.8MB/s \n","\u001b[?25hCollecting overrides==2.7.0\n","  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n","Collecting pandas==0.25.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n","\u001b[K     |████████████████████████████████| 10.4MB 14.6MB/s \n","\u001b[?25hCollecting pydantic==1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/24/e78cf017628e7eaed20cb040999b1ecc69f872da53dfd0d9aed40c0fa5f1/pydantic-1.3-cp36-cp36m-manylinux2010_x86_64.whl (7.3MB)\n","\u001b[K     |████████████████████████████████| 7.3MB 15.5MB/s \n","\u001b[?25hCollecting numpy==1.18.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/45f71bd24f4e37629e9db5fb75caab919507deae6a5a257f9e4685a5f931/numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n","\u001b[K     |████████████████████████████████| 20.1MB 1.3MB/s \n","\u001b[?25hCollecting rusenttokenize==0.0.5\n","  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n","Collecting pymorphy2==0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n","\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.4.1)\n","Collecting pyopenssl==19.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n","\u001b[?25hCollecting pymorphy2-dicts-ru\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n","\u001b[K     |████████████████████████████████| 8.0MB 30.5MB/s \n","\u001b[?25hCollecting nltk==3.4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 37.5MB/s \n","\u001b[?25hCollecting aio-pika==6.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n","\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.10.0)\n","Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (7.1.2)\n","Collecting ruamel.yaml==0.15.100\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/9f/83bb34eaf84032b0b54fcc4a6aff1858572d279d65a301c7ae875f523df5/ruamel.yaml-0.15.100-cp36-cp36m-manylinux1_x86_64.whl (656kB)\n","\u001b[K     |████████████████████████████████| 665kB 38.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (4.41.1)\n","Collecting Cython==0.29.14\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/d1/4d3f8a7a920e805488a966cc6ab55c978a712240f584445d703c08b9f405/Cython-0.29.14-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 46.2MB/s \n","\u001b[?25hCollecting pytz==2019.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n","\u001b[K     |████████████████████████████████| 512kB 41.5MB/s \n","\u001b[?25hCollecting scikit-learn==0.21.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 13.9MB/s \n","\u001b[?25hCollecting pytelegrambotapi==3.6.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n","\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n","\u001b[?25hCollecting fastapi==0.47.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n","\u001b[?25hCollecting requests==2.22.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n","\u001b[?25hCollecting sacremoses==0.0.35\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 40.5MB/s \n","\u001b[?25hCollecting uvicorn==0.11.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/5f/2bc87272f189662e129ddcd4807ad3ef83128b4df3a3482335f5f9790f24/uvicorn-0.11.7-py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n","Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic==1.3->deeppavlov) (0.7)\n","Collecting dawg-python>=0.7\n","  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n","Collecting pymorphy2-dicts<3.0,>=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 9.7MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n","Collecting cryptography>=2.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/62/30f6936941d87a5ed72efb24249437824f6b2c953901245b58c91fde2f27/cryptography-3.1.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 41.0MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from pyopenssl==19.1.0->deeppavlov) (1.15.0)\n","Collecting yarl\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c9/379b807a9c298b9694d0af8ee4260be7d40ab1a11fb9d4ae9e70b1e69d96/yarl-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (257kB)\n","\u001b[K     |████████████████████████████████| 266kB 48.5MB/s \n","\u001b[?25hCollecting aiormq<4,>=3.2.0\n","  Downloading https://files.pythonhosted.org/packages/6b/c6/4a9f8f22eef268289e9af5da6a620d837c700b333eae01132bfe48fe7dc9/aiormq-3.2.3-py3-none-any.whl\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.2->deeppavlov) (0.16.0)\n","Collecting starlette<=0.12.9,>=0.12.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n","Collecting idna<2.9,>=2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n","\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n","Collecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/48/586225bbb02d3bdca475b17e4be5ce5b3f09da2d6979f359916c1592a687/uvloop-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (3.9MB)\n","\u001b[K     |████████████████████████████████| 3.9MB 42.6MB/s \n","\u001b[?25hCollecting websockets==8.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n","\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n","\u001b[?25hCollecting h11<0.10,>=0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n","\u001b[?25hCollecting httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/dc1e7e8f4049ab70d52c9690ec10652e268ab2542853033cc1d539594102/httptools-0.1.1-cp36-cp36m-manylinux1_x86_64.whl (216kB)\n","\u001b[K     |████████████████████████████████| 225kB 51.1MB/s \n","\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.2)\n","Collecting multidict>=4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n","\u001b[K     |████████████████████████████████| 153kB 42.2MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.7.4.3)\n","Collecting pamqp==2.3.0\n","  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n","Building wheels for collected packages: overrides, nltk, pytelegrambotapi, sacremoses, starlette\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-2.7.0-cp36-none-any.whl size=5601 sha256=00ef42e8220eace7153a1bb54ea12591ea8867ed760cb2e4a902ce9195672b06\n","  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449904 sha256=5079b9af07020073c47ffcf9ad336f284582a346fe8c844ae5cd8ceeecd33a5c\n","  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n","  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp36-none-any.whl size=47179 sha256=966d327fc4805aca934435f3b4807cd6d1465e12d13610a782896ffcefb5d347\n","  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=4b4d00fa09ae206d918380155df156f57403a71852700404a4bc9277c12a57a8\n","  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n","  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for starlette: filename=starlette-0.12.9-cp36-none-any.whl size=57245 sha256=f23c220329a886319bf9be3fbb9d95d0f84f9fd5b191778966301e67d68fea31\n","  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n","Successfully built overrides nltk pytelegrambotapi sacremoses starlette\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: overrides, numpy, pytz, pandas, pydantic, rusenttokenize, dawg-python, pymorphy2-dicts, pymorphy2, cryptography, pyopenssl, pymorphy2-dicts-ru, nltk, multidict, idna, yarl, pamqp, aiormq, aio-pika, ruamel.yaml, Cython, scikit-learn, requests, pytelegrambotapi, starlette, fastapi, sacremoses, uvloop, websockets, h11, httptools, uvicorn, deeppavlov\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Found existing installation: pandas 1.0.5\n","    Uninstalling pandas-1.0.5:\n","      Successfully uninstalled pandas-1.0.5\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","  Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Found existing installation: Cython 0.29.21\n","    Uninstalling Cython-0.29.21:\n","      Successfully uninstalled Cython-0.29.21\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.2.3 cryptography-3.1.1 dawg-python-0.7.2 deeppavlov-0.12.1 fastapi-0.47.1 h11-0.9.0 httptools-0.1.1 idna-2.8 multidict-4.7.6 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1 yarl-1.6.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","pandas","pytz"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["2020-09-25 12:16:28.821 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes_bert' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/ner/ner_ontonotes_bert.json'\n","Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n","  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-ljpxptx8\n","  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-ljpxptx8\n","Building wheels for collected packages: bert-dp\n","  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-dp: filename=bert_dp-1.0-cp36-none-any.whl size=23581 sha256=239d9cd4294145f14ef4c80fc091c1819d636f7f61f6a729b956c865de0723b6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-w4gnga_k/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n","Successfully built bert-dp\n","Installing collected packages: bert-dp\n","Successfully installed bert-dp-1.0\n","Collecting tensorflow==1.15.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n","\u001b[K     |████████████████████████████████| 110.5MB 83kB/s \n","\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', OSError(\"(104, 'ECONNRESET')\",))': /simple/gast/\u001b[0m\n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 39.4MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.32.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.35.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.0)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 41.4MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.10.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.12.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (50.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.7.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=7d7bc9b6dbd6741d8321f5c0962c066a482fa47561241ea5431d6ea1bf038ede\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: gast, tensorflow-estimator, keras-applications, tensorboard, tensorflow\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: tensorflow 2.3.0\n","    Uninstalling tensorflow-2.3.0:\n","      Successfully uninstalled tensorflow-2.3.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SOcfbfDROwD9","colab_type":"text"},"source":["#Import Libraries and Model Configs"]},{"cell_type":"code","metadata":{"id":"ISPthiKMOZr9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1601036828636,"user_tz":-330,"elapsed":567383,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"f27dbd28-4f5e-4b1c-fc3d-30cbf61483ca"},"source":["from deeppavlov import configs, build_model\n","\n","ner_model = build_model(configs.ner.ner_ontonotes_bert, download=True)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2020-09-25 12:17:44.206 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/ner_ontonotes_bert_v1.tar.gz to /root/.deeppavlov/ner_ontonotes_bert_v1.tar.gz\n","100%|██████████| 805M/805M [05:05<00:00, 2.64MB/s]\n","2020-09-25 12:22:50.614 INFO in 'deeppavlov.core.data.utils'['utils'] at line 269: Extracting /root/.deeppavlov/ner_ontonotes_bert_v1.tar.gz archive into /root/.deeppavlov/models\n","2020-09-25 12:23:00.864 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip to /root/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip\n","100%|██████████| 404M/404M [03:27<00:00, 1.95MB/s]\n","2020-09-25 12:26:28.557 INFO in 'deeppavlov.core.data.utils'['utils'] at line 269: Extracting /root/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip archive into /root/.deeppavlov/downloads/bert_models\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package perluniprops to /root/nltk_data...\n","[nltk_data]   Unzipping misc/perluniprops.zip.\n","[nltk_data] Downloading package nonbreaking_prefixes to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["2020-09-25 12:26:37.15 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_ontonotes_bert/tag.dict]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n","\n","WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stdout"},{"output_type":"stream","text":["2020-09-25 12:27:05.456 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/ner_ontonotes_bert/model]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/ner_ontonotes_bert/model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"38o90kUgO3Uh","colab_type":"text"},"source":["#Inference"]},{"cell_type":"code","metadata":{"id":"Tnthn99lOfW7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":836},"executionInfo":{"status":"ok","timestamp":1601037609389,"user_tz":-330,"elapsed":2993,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"74feae7f-955e-443f-af9b-b7cf8bcd0a13"},"source":["ner_model(['The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I.']) #ner_model([Text])"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[['The',\n","   'Strawberry',\n","   'Twizzlers',\n","   'are',\n","   'my',\n","   'guilty',\n","   'pleasure',\n","   '-',\n","   'yummy',\n","   '.',\n","   'Six',\n","   'pounds',\n","   'will',\n","   'be',\n","   'around',\n","   'for',\n","   'a',\n","   'while',\n","   'with',\n","   'my',\n","   'son',\n","   'and',\n","   'I',\n","   '.']],\n"," [['B-WORK_OF_ART',\n","   'I-WORK_OF_ART',\n","   'I-WORK_OF_ART',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'B-QUANTITY',\n","   'I-QUANTITY',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O',\n","   'O']]]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"FTpa6HhTO52k","colab_type":"text"},"source":["#Code in required format"]},{"cell_type":"code","metadata":{"id":"CEEr17ZKOqEw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601037612124,"user_tz":-330,"elapsed":1343,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"e9d26c7e-11ec-4923-a3bb-5757dcf9f428"},"source":["ner=[]\n","#for i in range(80):\n","  #x = ner_model([df.iloc[i,-3]]) #ner_model([Text])\n","  #print(i)\n","  #flag=0\n","l=\"\"\n","x = ner_model(['The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I.']) #ner_model([Text]) #ner_model([Text])\n","for j in range(len(x[1][0])):\n","  if(x[1][0][j] == 'B-WORK_OF_ART' or x[1][0][j] == 'I-WORK_OF_ART' or x[1][0][j] == 'B-PRODUCT' or x[1][0][j] == 'I-PRODUCT'):  \n","      l = \" \" + x[0][0][j]\n","        #flag=1\n","    \n","  #if(flag==0):\n","    #l.append(\"\")\n","  #ner.append(l)\n","\n","print(l)\n","#L stores (Name of Product)\n","#Commented code can be used to run code through dataframe and to append results to DataFrame, Take care of indentation"],"execution_count":3,"outputs":[{"output_type":"stream","text":[" Twizzlers\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZDoATSBqtkgr","colab_type":"text"},"source":["##Keyword Extraction using TextRank Algorithm"]},{"cell_type":"code","metadata":{"id":"ODuqgodapqII","colab_type":"code","colab":{}},"source":["from collections import OrderedDict\n","import numpy as np\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","class TextRank4Keyword():\n","    \"\"\"Extract keywords from text\"\"\"\n","    \n","    def __init__(self):\n","        self.d = 0.85 # damping coefficient, usually is .85\n","        self.min_diff = 1e-5 # convergence threshold\n","        self.steps = 10 # iteration steps\n","        self.node_weight = None # save keywords and its weight\n","\n","    \n","    def set_stopwords(self, stopwords):  \n","        \"\"\"Set stop words\"\"\"\n","        for word in STOP_WORDS.union(set(stopwords)):\n","            lexeme = nlp.vocab[word]\n","            lexeme.is_stop = True\n","    \n","    def sentence_segment(self, doc, candidate_pos, lower):\n","        \"\"\"Store those words only in cadidate_pos\"\"\"\n","        sentences = []\n","        for sent in doc.sents:\n","            selected_words = []\n","            for token in sent:\n","                # Store words only with cadidate POS tag\n","                if token.pos_ in candidate_pos and token.is_stop is False:\n","                    if lower is True:\n","                        selected_words.append(token.text.lower())\n","                    else:\n","                        selected_words.append(token.text)\n","            sentences.append(selected_words)\n","        return sentences\n","        \n","    def get_vocab(self, sentences):\n","        \"\"\"Get all tokens\"\"\"\n","        vocab = OrderedDict()\n","        i = 0\n","        for sentence in sentences:\n","            for word in sentence:\n","                if word not in vocab:\n","                    vocab[word] = i\n","                    i += 1\n","        return vocab\n","    \n","    def get_token_pairs(self, window_size, sentences):\n","        \"\"\"Build token_pairs from windows in sentences\"\"\"\n","        token_pairs = list()\n","        for sentence in sentences:\n","            for i, word in enumerate(sentence):\n","                for j in range(i+1, i+window_size):\n","                    if j >= len(sentence):\n","                        break\n","                    pair = (word, sentence[j])\n","                    if pair not in token_pairs:\n","                        token_pairs.append(pair)\n","        return token_pairs\n","        \n","    def symmetrize(self, a):\n","        return a + a.T - np.diag(a.diagonal())\n","    \n","    def get_matrix(self, vocab, token_pairs):\n","        \"\"\"Get normalized matrix\"\"\"\n","        # Build matrix\n","        vocab_size = len(vocab)\n","        g = np.zeros((vocab_size, vocab_size), dtype='float')\n","        for word1, word2 in token_pairs:\n","            i, j = vocab[word1], vocab[word2]\n","            g[i][j] = 1\n","            \n","        # Get Symmeric matrix\n","        g = self.symmetrize(g)\n","        \n","        # Normalize matrix by column\n","        norm = np.sum(g, axis=0)\n","        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n","        \n","        return g_norm\n","\n","    \n","    def get_keywords(self, number=10):\n","        \"\"\"Print top number keywords\"\"\"\n","        ls = []\n","        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","        for i, (key, value) in enumerate(node_weight.items()):\n","            ls.append(key + ' - ' + str(value))\n","            if i > number:\n","                break\n","        return ls\n","        \n","        \n","    def analyze(self, text, \n","                candidate_pos=['NOUN', 'PROPN'], \n","                window_size=4, lower=False, stopwords=list()):\n","        \"\"\"Main function to analyze text\"\"\"\n","        \n","        # Set stop words\n","        self.set_stopwords(stopwords)\n","        \n","        # Pare text by spaCy\n","        doc = nlp(text)\n","        \n","        # Filter sentences\n","        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n","        \n","        # Build vocabulary\n","        vocab = self.get_vocab(sentences)\n","        \n","        # Get token_pairs from windows\n","        token_pairs = self.get_token_pairs(window_size, sentences)\n","        \n","        # Get normalized matrix\n","        g = self.get_matrix(vocab, token_pairs)\n","        \n","        # Initionlization for weight(pagerank value)\n","        pr = np.array([1] * len(vocab))\n","        \n","        # Iteration\n","        previous_pr = 0\n","        for epoch in range(self.steps):\n","            pr = (1-self.d) + self.d * np.dot(g, pr)\n","            if abs(previous_pr - sum(pr))  < self.min_diff:\n","                break\n","            else:\n","                previous_pr = sum(pr)\n","\n","        # Get weight for each node\n","        node_weight = dict()\n","        for word, index in vocab.items():\n","            node_weight[word] = pr[index]\n","        \n","        self.node_weight = node_weight"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8Solo7ytuJX","colab_type":"code","colab":{}},"source":["tr4w = TextRank4Keyword()\n","tr4w.analyze(text, candidate_pos = ['NOUN'], window_size=4, lower=False)\n","#kw.append(tr4w.get_keywords(10))\n","tr4w.get_keywords(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAHqKfAYuwlH","colab_type":"text"},"source":["#To perform on a complete dataset and then store in a dataframe"]},{"cell_type":"code","metadata":{"id":"UWrrnQFHuqDW","colab_type":"code","colab":{}},"source":["kw = []\n","for i in range(len(preprocess)):\n","  tr4w = TextRank4Keyword()\n","  tr4w.analyze(preprocess[i], candidate_pos = ['NOUN'], window_size=4, lower=False)\n","  kw.append(tr4w.get_keywords(10))\n","  #tr4w.get_keywords(10)\n","\n","df_keywords_tr = pd.DataFrame()\n","df_keywords_tr['Text'] = preprocess\n","df_keywords_tr['Keywords'] = kw\n","df_keywords_tr.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMXHtsAPvc12","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}