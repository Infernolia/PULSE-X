{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Saloni_BERT_2.ipynb","provenance":[],"authorship_tag":"ABX9TyMod2MI2wx51d8405yUD+Uw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PAEsP2HAhDTf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"status":"ok","timestamp":1600698285171,"user_tz":-330,"elapsed":4044,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"b996bdf9-b303-412d-a620-71daa4833442"},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertConfig\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","torch.__version__"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.6.0+cu101'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"6G67pHJ9hTZ1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":666},"executionInfo":{"status":"ok","timestamp":1600697732491,"user_tz":-330,"elapsed":9828,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"d4dcb248-1cca-4d6e-e199-d6496a9f9aa4"},"source":["!pip install pytorch-pretrained-bert\n","!pip install seqeval"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\r\u001b[K     |██▋                             | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.6.0+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.59)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n","Requirement already satisfied: botocore<1.18.0,>=1.17.59 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.17.59)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->pytorch-pretrained-bert) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->pytorch-pretrained-bert) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.59->boto3->pytorch-pretrained-bert) (1.15.0)\n","Installing collected packages: pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2\n","Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.2.4->seqeval) (1.15.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=018b1e345429de7bb569f611f75cb488179c80882a361e9b1900732fc687fc3a\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-0.0.12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PWPKZPMDikpO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":360},"executionInfo":{"status":"ok","timestamp":1600697724429,"user_tz":-330,"elapsed":3854,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"d1ba96dd-8480-43ff-b8a4-80c298d769b2"},"source":["!pip install transformers"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n","Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u4q53aHyhcuV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"status":"ok","timestamp":1600698292545,"user_tz":-330,"elapsed":2107,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"c8e62aa3-6e66-48a6-d8e6-de6feab093c4"},"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, trange\n","\n","data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")\n","data.tail(10)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1048565</th>\n","      <td>Sentence: 47958</td>\n","      <td>impact</td>\n","      <td>NN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1048566</th>\n","      <td>Sentence: 47958</td>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1048567</th>\n","      <td>Sentence: 47959</td>\n","      <td>Indian</td>\n","      <td>JJ</td>\n","      <td>B-gpe</td>\n","    </tr>\n","    <tr>\n","      <th>1048568</th>\n","      <td>Sentence: 47959</td>\n","      <td>forces</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1048569</th>\n","      <td>Sentence: 47959</td>\n","      <td>said</td>\n","      <td>VBD</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1048570</th>\n","      <td>Sentence: 47959</td>\n","      <td>they</td>\n","      <td>PRP</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1048571</th>\n","      <td>Sentence: 47959</td>\n","      <td>responded</td>\n","      <td>VBD</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1048572</th>\n","      <td>Sentence: 47959</td>\n","      <td>to</td>\n","      <td>TO</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1048573</th>\n","      <td>Sentence: 47959</td>\n","      <td>the</td>\n","      <td>DT</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1048574</th>\n","      <td>Sentence: 47959</td>\n","      <td>attack</td>\n","      <td>NN</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              Sentence #       Word  POS    Tag\n","1048565  Sentence: 47958     impact   NN      O\n","1048566  Sentence: 47958          .    .      O\n","1048567  Sentence: 47959     Indian   JJ  B-gpe\n","1048568  Sentence: 47959     forces  NNS      O\n","1048569  Sentence: 47959       said  VBD      O\n","1048570  Sentence: 47959       they  PRP      O\n","1048571  Sentence: 47959  responded  VBD      O\n","1048572  Sentence: 47959         to   TO      O\n","1048573  Sentence: 47959        the   DT      O\n","1048574  Sentence: 47959     attack   NN      O"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"Gio0EMbhhgE3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698295414,"user_tz":-330,"elapsed":1191,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["class SentenceGetter(object):\n","\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n","                                                           s[\"POS\"].values.tolist(),\n","                                                           s[\"Tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","\n","    def get_next(self):\n","        try:\n","            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQeqnbqnhiJ5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698303195,"user_tz":-330,"elapsed":6439,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["getter = SentenceGetter(data)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rlgh47gqhjcP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1600698306248,"user_tz":-330,"elapsed":1124,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"66dcc5c6-fbea-4b1f-b90b-803b2ebbe9cb"},"source":["sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n","sentences[0]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Thousands',\n"," 'of',\n"," 'demonstrators',\n"," 'have',\n"," 'marched',\n"," 'through',\n"," 'London',\n"," 'to',\n"," 'protest',\n"," 'the',\n"," 'war',\n"," 'in',\n"," 'Iraq',\n"," 'and',\n"," 'demand',\n"," 'the',\n"," 'withdrawal',\n"," 'of',\n"," 'British',\n"," 'troops',\n"," 'from',\n"," 'that',\n"," 'country',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"aXavODvihk7U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1600698309509,"user_tz":-330,"elapsed":1272,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"c271f3b2-22d9-4eb8-f0af-05f363d58253"},"source":["labels = [[s[2] for s in sentence] for sentence in getter.sentences]\n","print(labels[0])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2FHKS8pZhnBM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698312619,"user_tz":-330,"elapsed":1273,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["tag_values = list(set(data[\"Tag\"].values))\n","tag_values.append(\"PAD\")\n","tag2idx = {t: i for i, t in enumerate(tag_values)}\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpigql9thoaL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698316743,"user_tz":-330,"elapsed":1551,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["MAX_LEN = 75\n","bs = 32\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"bD8JrNfLhsNC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698319407,"user_tz":-330,"elapsed":749,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"kuQikBGdDhj_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600698331461,"user_tz":-330,"elapsed":1436,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"7a2ca1d3-37d4-4e98-93c7-09b5f7b9c799"},"source":["device"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"uiVdSe4YhtuV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":307},"executionInfo":{"status":"error","timestamp":1600697594913,"user_tz":-330,"elapsed":3069,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"c9b1e3d3-0cf6-4ce1-becb-8106628b2bc0"},"source":["torch.cuda.get_device_name(0)\n"],"execution_count":11,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-f9c260299c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_device_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m             raise AssertionError(\n\u001b[1;32m    189\u001b[0m                 \"libcudart functions unavailable. It looks like you have a broken build?\")\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:47"]}]},{"cell_type":"code","metadata":{"id":"XAeO9FJnhv1Y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698339286,"user_tz":-330,"elapsed":1628,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8LRNTftjYHO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698343648,"user_tz":-330,"elapsed":1404,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["def tokenize_and_preserve_labels(sentence, text_labels):\n","    tokenized_sentence = []\n","    labels = []\n","\n","    for word, label in zip(sentence, text_labels):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = tokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcinPXfAjaW5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698390243,"user_tz":-330,"elapsed":44035,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["tokenized_texts_and_labels = [\n","    tokenize_and_preserve_labels(sent, labs)\n","    for sent, labs in zip(sentences, labels)\n","]\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-9kVq8JjcYf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698733960,"user_tz":-330,"elapsed":1202,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n","labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"8imcxgP7jgFS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698739976,"user_tz":-330,"elapsed":2711,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n","                          truncating=\"post\", padding=\"post\")\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6Zxrg46jj45","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698742765,"user_tz":-330,"elapsed":1257,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"uX1T0bd4jq0h","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698753166,"user_tz":-330,"elapsed":9297,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"Te2_eNtNjtVt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698755595,"user_tz":-330,"elapsed":1134,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n","                                                            random_state=2018, test_size=0.1)\n","tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.1)\n"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"PaS2x0RSjvaS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698758427,"user_tz":-330,"elapsed":1337,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["tr_inputs = torch.tensor(tr_inputs)\n","val_inputs = torch.tensor(val_inputs)\n","tr_tags = torch.tensor(tr_tags)\n","val_tags = torch.tensor(val_tags)\n","tr_masks = torch.tensor(tr_masks)\n","val_masks = torch.tensor(val_masks)\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"3NdeNh8FjxBP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698760217,"user_tz":-330,"elapsed":761,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","\n","valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","valid_sampler = SequentialSampler(valid_data)\n","valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BSicW3ejzE3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"status":"ok","timestamp":1600698762361,"user_tz":-330,"elapsed":736,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"26c9bf6d-2089-4d19-b1f8-9496abd1b4f1"},"source":["import transformers\n","from transformers import BertForTokenClassification, AdamW\n","\n","transformers.__version__\n"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'3.1.0'"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"feChssamj05B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1600698770418,"user_tz":-330,"elapsed":6948,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"47412971-a487-4eb1-d8e6-4a352a68d1d5"},"source":["model = BertForTokenClassification.from_pretrained(\n","    \"bert-base-cased\",\n","    num_labels=len(tag2idx),\n","    output_attentions = False,\n","    output_hidden_states = False\n",")\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"s3suuqzsj5ir","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698802037,"user_tz":-330,"elapsed":709,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["FULL_FINETUNING = True\n","if FULL_FINETUNING:\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.0}\n","    ]\n","else:\n","    param_optimizer = list(model.classifier.named_parameters())\n","    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","\n","optimizer = AdamW(\n","    optimizer_grouped_parameters,\n","    lr=3e-5,\n","    eps=1e-8\n",")\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gnx8VQO6j_n_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698833699,"user_tz":-330,"elapsed":1622,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 3\n","max_grad_norm = 1.0\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"khqUVDdnF8QN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600698962726,"user_tz":-330,"elapsed":1326,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}}},"source":["from seqeval.metrics import f1_score, accuracy_score\n"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1-2hD4eGAVl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e187ce7c-9802-447a-d390-87916f5f04a0"},"source":["## Store the average loss after each epoch so we can plot them.\n","loss_values, validation_loss_values = [], []\n","\n","for _ in trange(epochs, desc=\"Epoch\"):\n","    # ========================================\n","    #               Training\n","    # ========================================\n","    # Perform one full pass over the training set.\n","\n","    # Put the model into training mode.\n","    model.train()\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Training loop\n","    for step, batch in enumerate(train_dataloader):\n","        # add batch to gpu\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Always clear any previously calculated gradients before performing a backward pass.\n","        model.zero_grad()\n","        # forward pass\n","        # This will return the loss (rather than the model output)\n","        # because we have provided the `labels`.\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                        attention_mask=b_input_mask, labels=b_labels)\n","        # get the loss\n","        loss = outputs[0]\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","        # track train loss\n","        total_loss += loss.item()\n","        # Clip the norm of the gradient\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","        # update parameters\n","        optimizer.step()\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(\"Average train loss: {}\".format(avg_train_loss))\n","\n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    # Put the model into evaluation mode\n","    model.eval()\n","    # Reset the validation loss for this epoch.\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    predictions , true_labels = [], []\n","    for batch in valid_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Telling the model not to compute or store gradients,\n","        # saving memory and speeding up validation\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have not provided labels.\n","            outputs = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask, labels=b_labels)\n","        # Move logits and labels to CPU\n","        logits = outputs[1].detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences.\n","        eval_loss += outputs[0].mean().item()\n","        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","        true_labels.extend(label_ids)\n","\n","    eval_loss = eval_loss / len(valid_dataloader)\n","    validation_loss_values.append(eval_loss)\n","    print(\"Validation loss: {}\".format(eval_loss))\n","    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n","                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n","    valid_tags = [tag_values[l_i] for l in true_labels\n","                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n","    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n","    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n","    print()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vIbdjhzKxsfY","colab_type":"code","colab":{}},"source":["test_sentence = 'The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I.'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEl_x6dExgu5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"error","timestamp":1600693663779,"user_tz":-330,"elapsed":938,"user":{"displayName":"Aboli Marathe","photoUrl":"","userId":"00115682461635775249"}},"outputId":"73f01ca3-f92f-411e-b6c6-54b1fb6fd2b4"},"source":["tokenized_sentence = tokenizer.encode(test_sentence)\n","input_ids = torch.tensor([tokenized_sentence]).cuda()\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-521e4566b730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}]},{"cell_type":"code","metadata":{"id":"oZpHD1uyxvKM","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}